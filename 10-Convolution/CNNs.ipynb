{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5995bdda",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2022/blob/main/10-convolution-networks/CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61921b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import cross_entropy, relu\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4ee1b",
   "metadata": {},
   "source": [
    "# Demonstration: convolving to extract features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a4ccd",
   "metadata": {},
   "source": [
    "Let's check out the image we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd53e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.load(\"img.npy\")\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.from_numpy(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2190891",
   "metadata": {},
   "source": [
    "At first, we'll experiment with `nn.functional.conv2d` - the function that performs 2d image convolution.\n",
    "\n",
    "Note: this function is designed to work in the context of a neural network (i.e. where input and output come in batches and have multiple channels), so the functin expects 4D tensors rather than 2D. We'll write a short wrapper to work with 2D images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b26e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(img, kernel):\n",
    "    kernel = kernel[None, None, ...]\n",
    "\n",
    "    result = nn.functional.conv2d(img[None, None, ...].float(), kernel)\n",
    "    \n",
    "    return result.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b3755",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_ver_edge = torch.tensor(\n",
    "    [[ 1., -1.],\n",
    "     [ 1., -1.]]\n",
    ")\n",
    "kernel_hor_edge = torch.tensor(\n",
    "    [[ 1.,  1.],\n",
    "     [-1., -1.]]\n",
    ")\n",
    "\n",
    "vertical_edges = convolve(img, kernel_ver_edge)\n",
    "horizontal_edges = convolve(img, kernel_hor_edge)\n",
    "\n",
    "plt.figure(figsize=(4, 5), dpi=150)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(vertical_edges);\n",
    "plt.colorbar()\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(horizontal_edges);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f858e5",
   "metadata": {},
   "source": [
    "We can combine the result, e.g. like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = (vertical_edges**2 + horizontal_edges**2)**0.5\n",
    "plt.figure(dpi=150)\n",
    "plt.imshow(edges);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fbbee7",
   "metadata": {},
   "source": [
    "Another example, blurring kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_blur = torch.tensor([[1.,  4.,  7.,  4., 1.],\n",
    "                                    [4., 16., 26., 16., 4.],\n",
    "                                    [7., 26., 41., 26., 7.],\n",
    "                                    [4., 16., 26., 16., 4.],\n",
    "                                    [1.,  4.,  7.,  4., 1.]]) / 273\n",
    "\n",
    "edges_blurred = convolve(img, kernel_blur)\n",
    "\n",
    "## Uncomment these lines one by one to see the effect\n",
    "## gradually increasing:\n",
    "edges_blurred = convolve(edges_blurred, kernel_blur)\n",
    "edges_blurred = convolve(edges_blurred, kernel_blur)\n",
    "edges_blurred = convolve(edges_blurred, kernel_blur)\n",
    "edges_blurred = convolve(edges_blurred, kernel_blur)\n",
    "## Keep them **uncommented** for the further code to work\n",
    "\n",
    "plt.imshow(edges_blurred.detach().numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930f271b",
   "metadata": {},
   "source": [
    "Let's pick up a small patch out of this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d90cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_subset = edges_blurred[210:243, 246:282]\n",
    "plt.imshow(edges_subset);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9ca73",
   "metadata": {},
   "source": [
    "What do you think will happen if we use this patch as a kernel when running convolution on the edges image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb360db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.imshow(convolve(edges_blurred, edges_subset))\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22feea94",
   "metadata": {},
   "source": [
    "Note how this kernel highlighted the location of that shape on the input!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df6298",
   "metadata": {},
   "source": [
    "We can either have a look at kernels of pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b092af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_kernels(conv_layer, num_cols=8):\n",
    "    kernels = conv_layer.weight.data\n",
    "    num_rows = int(np.ceil(len(kernels) / num_cols))\n",
    "    plt.figure(figsize=(12, 12 // num_cols * num_rows))\n",
    "    \n",
    "    for i, w in enumerate(kernels, 1):\n",
    "        w = w.cpu().numpy().transpose(1, 2, 0)\n",
    "        w -= w.min()\n",
    "        w /= w.max()\n",
    "        \n",
    "        plt.subplot(num_rows, num_cols, i)\n",
    "        plt.imshow(w)\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_pt = resnet18(pretrained=True)\n",
    "show_kernels(resnet18_pt.conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb4e04b",
   "metadata": {},
   "source": [
    "Now let's build up a ResNet model, using Conv layers, and compare it's performance depending on batch norm and residual blocks. Based on a great [VK CV course](https://github.com/lysukhin/vk-academy-dl-cv/tree/2022/computer_vision) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b828263-ea0f-4394-9b01-ee3e6417e2d6",
   "metadata": {},
   "source": [
    "# CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b064ff2",
   "metadata": {},
   "source": [
    "![kernel](https://miro.medium.com/max/640/1*1okwhewf5KCtIPaFib4XaA.gif)\n",
    "\n",
    "\n",
    "* stride controls the stride for the cross-correlation, a single number or a tuple.\n",
    "\n",
    "* padding controls the amount of padding applied to the input. It can be either a string {‘valid’, ‘same’} or a tuple of ints giving the amount of implicit padding applied on both sides.\n",
    "\n",
    "* dilation controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.\n",
    "\n",
    "You may check other visualizations [here](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4caba",
   "metadata": {},
   "source": [
    "## Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d10e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(arr, label=\"\"):\n",
    "    plt.plot(arr, label=label)\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"CE loss\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd033381-342e-4fe7-83bf-ac7982effff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"CIFAR10\"\n",
    "DEVICE = \"cuda:0\"\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 256\n",
    "LR = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca3a23-c307-49e7-869a-923e5e91c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(0.5, 0.25, inplace=True),\n",
    "])\n",
    "\n",
    "IMAGE_CHANNELS = 3\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "dataset = CIFAR10(\"./dataset/cifar\", download=True, transform=transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b81a4b0-e74d-4c01-a75e-ced5da1f595d",
   "metadata": {},
   "source": [
    "Complete the code below, following function's docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ed23c-f3d3-452b-8a45-97856a1e3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                dataset=dataset,\n",
    "                num_epochs=NUM_EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                lr=LR,\n",
    "                device=DEVICE):\n",
    "    \"\"\"Model training routine function. \n",
    "    Uses Adam optimizer & cross-entropy loss.\n",
    "    \n",
    "    Args:\n",
    "        model: torch.nn.Module\n",
    "        dataset: torch.utils.data.Dataset\n",
    "        num_epochs: int\n",
    "        batch_size: int\n",
    "        lr: float\n",
    "        device: str\n",
    "        \n",
    "    Returns:\n",
    "        losses: list of float values of length num_epochs * len(dataloader)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    dataloader = # YOUR CODE HERE \n",
    "    \n",
    "    optimizer = # YOUR CODE HERE \n",
    "    \n",
    "    losses = []    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in tqdm.tqdm(dataloader):\n",
    "            xs, ys_true = batch\n",
    "            \n",
    "            logits_pred = # YOUR CODE HERE \n",
    "            \n",
    "            loss = # YOUR CODE HERE\n",
    "            \n",
    "            # optimization step\n",
    "            # YOUR CODE HERE\n",
    "                                \n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbb8b9-d573-49dd-9d5f-63e93c16b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS for train()\n",
    "\n",
    "input_size = dataset[0][0].size()\n",
    "dummy_model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(np.prod(input_size), NUM_CLASSES)\n",
    ")\n",
    "losses_dummy = train_model(dummy_model, dataset, num_epochs=1)\n",
    "\n",
    "assert len(losses_dummy) == len(dataset) // BATCH_SIZE\n",
    "assert np.mean(losses_dummy[:10]) > np.mean(losses_dummy[-10:])\n",
    "\n",
    "plot(losses_dummy, \"Dummy Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8823fc3d-f131-4e29-b52d-639e2e155cce",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e932676-561e-4dfb-9dfd-852defb8fdbb",
   "metadata": {},
   "source": [
    "[ResNet](https://arxiv.org/abs/1512.03385) architecture:\n",
    "\n",
    "![resnet34](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/resnet_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a87f05",
   "metadata": {},
   "source": [
    "You can find exact impementation inside [torchvision code](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py), it may be helpful for you during other experiments as you can check how some particular model is implemented. However, let's try to create our own ResNet with 10 layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83960262-e713-43f9-88de-1caad2bd2bb8",
   "metadata": {},
   "source": [
    "As we are going to use `Conv2` -> `ReLU` -> `BatchNorm2d` sequence way too often, let's implement a function to create such a sequence. Note, that convolutional layer musts preserve spatial tensor dims (i.e. apply zero padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa8389-cd0c-4f98-9ab6-abde4fdfc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv(kernel_size,\n",
    "             in_features,\n",
    "             out_features,\n",
    "             with_bn=True,\n",
    "             with_relu=True):\n",
    "    \"\"\"Create conv -> [relu] -> [bn] layers, embedded in torch.nn.Sequential module.\n",
    "    \n",
    "    ! Conv layer must preserve spatial tensor dims (i.e. apply zero padding).\n",
    "    \n",
    "    Args:\n",
    "        kernel_size: int\n",
    "        in_features: int\n",
    "        out_features: int\n",
    "        with_bn: bool\n",
    "        with_relu: bool\n",
    "        \n",
    "    Returns:\n",
    "        torch.nn.Sequential\n",
    "    \"\"\"\n",
    "    layers = [\n",
    "        nn.Conv2d(in_features, out_features, kernel_size, 1, kernel_size // 2)\n",
    "    ]\n",
    "    \n",
    "    # replace with ReLU6 from nn.functional.relu6 \n",
    "    # if you face problem on colab\n",
    "    if with_relu:\n",
    "        layers.append(nn.ReLU(inplace=(with_bn is True)))\n",
    "        \n",
    "    if with_bn:\n",
    "        layers.append(nn.BatchNorm2d(out_features))\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56c194-254d-4379-ac84-d200d1b2b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS for get_conv()\n",
    "conv = get_conv(3, 8, 16)\n",
    "\n",
    "assert len(conv) == 3\n",
    "assert isinstance(conv[0], torch.nn.Conv2d)\n",
    "assert conv[0].in_channels == 8\n",
    "assert conv[0].out_channels == 16\n",
    "\n",
    "assert isinstance(conv[1], torch.nn.ReLU)\n",
    "\n",
    "assert isinstance(conv[2], torch.nn.BatchNorm2d)\n",
    "assert conv[2].num_features == 16\n",
    "\n",
    "\n",
    "conv = get_conv(3, 8, 16, with_bn=False, with_relu=False)\n",
    "\n",
    "assert len(conv) == 1\n",
    "assert isinstance(conv[0], torch.nn.Conv2d)\n",
    "assert conv[0].in_channels == 8\n",
    "assert conv[0].out_channels == 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a694b-b778-4714-8262-4a86467ed7b7",
   "metadata": {},
   "source": [
    "Global Average Pooling is a pooling operation designed to replace fully connected layers in classical CNNs. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the **average of each feature map**.\n",
    "\n",
    "One advantage of global average pooling over the fully connected layers is that it is more native to the convolution structure by enforcing correspondences between feature maps and categories. Thus the feature maps can be easily interpreted as categories confidence maps.\n",
    "Another advantage is that there is **no parameter to optimize in the global average pooling** thus overfitting is avoided at this layer. Furthermore, global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.\n",
    "\n",
    "Implement `GlobalAveragePooling2d` class to use it in ResNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741a780-218f-45f1-bc57-19871d620832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAveragePooling2d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        \"\"\"GAP forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: torch.Tensor, size B x C x H x W.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor, size B x C.\n",
    "        \"\"\"\n",
    "        # y = # YOUR CODE HERE\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a41766-4ede-4944-8ace-c09732e87a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS for GAP\n",
    "\n",
    "gap = GlobalAveragePooling2d()\n",
    "x = torch.randn(4, 3, 16, 16)\n",
    "y = gap(x)\n",
    "\n",
    "assert y.size() == (4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3a3ef-ae94-46e5-a93f-3d10e14c6f59",
   "metadata": {},
   "source": [
    "## Residual Block\n",
    "\n",
    "In traditional neural networks, each layer feeds into the next layer. In a network with residual blocks, each layer feeds into the next layer and directly into the layers about 2–3 hops away. \n",
    "\n",
    "\n",
    "![residual_block](https://miro.medium.com/max/720/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)\n",
    "\n",
    "\n",
    "Note, that  `x` and `F(x)` may have different shapes.\n",
    "We can also redefine `__repr__` to may the `print` output fancier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d351646-d5db-4874-aa45-c5d8ecb9203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, num_input_features, num_features, num_layers, with_bn=True, residual=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if residual:\n",
    "            if num_input_features != num_features:\n",
    "                self.projection = nn.Conv2d(num_input_features, num_features, 1, 1, 0)\n",
    "            else:\n",
    "                self.projection = None\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            conv = get_conv(3, num_input_features, num_features, with_bn=with_bn)\n",
    "            layers.append(conv)\n",
    "            num_input_features = num_features\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.num_input_features = num_input_features\n",
    "        self.num_features = num_features\n",
    "        self.num_layers = num_layers\n",
    "        self.with_bn = with_bn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        Applies convolution layers and skip-connection; self.projection, if necessary.\n",
    "        \n",
    "        Args:\n",
    "            x: torch.Tensor, size B x C x H x W.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor, size B x C x H x W.\n",
    "        \"\"\"\n",
    "        x_input = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        if self.projection is not None:\n",
    "            x += self.projection(x_input)\n",
    "        else:\n",
    "            x += x_input\n",
    "            \n",
    "        return relu(x)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        out = f\"ResidualBlock(num_input_features={self.num_input_features}, num_features={self.num_features}, num_layers={self.num_layers}, with_bn={self.with_bn})\"\n",
    "        for l in self.layers:\n",
    "            out += \"\\n\" + \"\\t\" + repr(l)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8f2f3-17f2-4f53-a914-160e6314f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS for ResidualBlock\n",
    "block = ResidualBlock(4, 4, 2)\n",
    "\n",
    "assert len(block.layers) == 2\n",
    "assert len(block.layers[0]) == 3\n",
    "assert len(block.layers[1]) == 3\n",
    "assert isinstance(block.layers[1][2], nn.BatchNorm2d)\n",
    "\n",
    "print(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09efc02b-f4f6-46a2-86df-336e61fa8be3",
   "metadata": {},
   "source": [
    "Finally, let's combine it and implement the `build_resnet` function, as it would be easier to call it instead of copypasting through our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa5aec0-062c-4474-9876-5c4fff5ef96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(num_input_features=IMAGE_CHANNELS,\n",
    "                 num_classes=NUM_CLASSES,\n",
    "                 with_bn=True,\n",
    "                 block = ResidualBlock):\n",
    "    \n",
    "    pool = nn.MaxPool2d((2, 2))\n",
    "    gap = GlobalAveragePooling2d()\n",
    "    fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    resnet = nn.Sequential(\n",
    "        get_conv(7, num_input_features, 64, with_bn=with_bn),\n",
    "        pool,\n",
    "        block(64, 64, 2, with_bn=with_bn),\n",
    "        pool,\n",
    "        block(64, 128, 2, with_bn=with_bn),\n",
    "        pool,\n",
    "        block(128, 256, 2, with_bn=with_bn),\n",
    "        pool,\n",
    "        block(256, 512, 2, with_bn=with_bn),\n",
    "        gap,\n",
    "        fc\n",
    "    )\n",
    "    \n",
    "    return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf666bd-64dc-4c05-a300-55c75f43b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = build_resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7f0ae-2ba0-4e93-b16e-1f429b51b9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_resnet= train_model(resnet, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a7f136-7774-4575-b60c-b56b562f7831",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(losses_resnet, label=\"ResNet10 with Batch Norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a3065-42d2-4944-adb5-e11a57f6afd5",
   "metadata": {},
   "source": [
    "## Batch Normalization Effect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb573b7-d7e1-4a2f-b56e-4584fb58f146",
   "metadata": {},
   "source": [
    "Now, we create another instance of ResNet but do not use batch normalization. So, let's check how it affects the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698ca43-33c6-4729-9bcb-6bcf507c9f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_wo_bn = build_resnet(with_bn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba29a2-3af7-4a68-bb64-37881cd84513",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_resnet_wo_bn = train_model(resnet_wo_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1029d-6da9-449e-b87b-8bd5211dc427",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(losses_resnet, label=\"Batch Norm\")\n",
    "plot(losses_resnet_wo_bn, label=\"Without Batch Norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c3b7b-ed47-4dc7-a68f-2ef706b5d402",
   "metadata": {},
   "source": [
    "## Residual-connections Effect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fad0f6-fecd-49b7-b8be-61418f2055bd",
   "metadata": {},
   "source": [
    "Now we get rid of residual connections and compare the results. We may reuse the previous class, deleting everything redundant.🤡 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1530e61-67fc-4cfd-a432-72c44e9ef4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(ResidualBlock):\n",
    "    def __init__(self, num_input_features, num_features, num_layers, with_bn=True):\n",
    "        super().__init__(num_input_features, num_features, num_layers, with_bn, False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        out = f\"Block(num_input_features={self.num_input_features}, num_features={self.num_features}, num_layers={self.num_layers}, with_bn={self.with_bn})\"\n",
    "        for l in self.layers:\n",
    "            out += \"\\n\" + \"\\t\" + repr(l)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cce5ca7-8b9c-4e56-8f9e-01bcdc74d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "net10 = build_resnet(block=Block)\n",
    "net10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45b91d-1f74-4509-9bae-b5d1ed1320c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_net = train_model(net10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13dc210-8717-40b7-b8b8-400d19dc1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(losses_resnet, label=\"resnet10\")\n",
    "plot(losses_net, label=\"net10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c51f1d-f57e-4833-b660-d893f9686ce9",
   "metadata": {},
   "source": [
    "Why do we see no difference here?\n",
    "\n",
    "Looks like we need to go deeper. Let's increase the number of blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(num_input_features=IMAGE_CHANNELS,\n",
    "                 num_classes=NUM_CLASSES,\n",
    "                 with_bn=True,\n",
    "                 block = ResidualBlock):\n",
    "    \n",
    "    pool = nn.MaxPool2d((2, 2))\n",
    "    gap = GlobalAveragePooling2d()\n",
    "    fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    resnet = nn.Sequential(\n",
    "        get_conv(7, num_input_features, 64, with_bn=with_bn),\n",
    "        pool,\n",
    "        block(64, 64, 2, with_bn=with_bn),\n",
    "        block(64, 64, 2, with_bn=with_bn),\n",
    "        block(64, 64, 2, with_bn=with_bn),\n",
    "        pool,\n",
    "        block(64, 128, 2, with_bn=with_bn),\n",
    "        block(128, 128, 2, with_bn=with_bn),\n",
    "        block(128, 128, 2, with_bn=with_bn),\n",
    "        pool,\n",
    "        block(128, 256, 2, with_bn=with_bn),\n",
    "        block(256, 256, 2, with_bn=with_bn),\n",
    "        block(256, 256, 2, with_bn=with_bn),\n",
    "        block(256, 256, 2, with_bn=with_bn),\n",
    "        block(256, 256, 2, with_bn=with_bn),\n",
    "        pool,\n",
    "        block(256, 512, 2, with_bn=with_bn),\n",
    "        block(512, 512, 2, with_bn=with_bn),\n",
    "        block(512, 512, 2, with_bn=with_bn),\n",
    "        gap,\n",
    "        fc\n",
    "    )\n",
    "    \n",
    "    return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26014f73-120c-4790-9322-00685a66e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_34 = build_resnet(block=ResidualBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb9f14-e01c-4b1f-86ea-a5fd047593eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_resnet_34 = train_model(resnet_34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d162da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_34 = build_resnet(block=Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_model_34 = train_model(model_34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0fcee6-7987-4f66-9728-c43454f54518",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(losses_resnet_34 , \"ResNet\")\n",
    "plot(losses_model_34, \"No Residual Blocks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
