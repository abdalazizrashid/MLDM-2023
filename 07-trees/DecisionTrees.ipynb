{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2022/blob/master/07-trees/DecisionTrees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-MpKxz14SSU"
   },
   "source": [
    "# Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWa4yz4qbxkF"
   },
   "source": [
    "![img](https://pbs.twimg.com/media/B13n2VVCIAA0hJS.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EBDdjJ-ay0M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zguy6o5Pb3-x"
   },
   "source": [
    "Let's generate a toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdtf3-9WauYy"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_toy, y_toy = make_blobs(n_samples=400,\n",
    "                          centers=[[0., 1.], [1., 2.]],\n",
    "                          random_state=14)\n",
    "\n",
    "plt.scatter(X_toy[:, 0], X_toy[:, 1], c=y_toy, alpha=0.8, cmap='bwr')\n",
    "plt.xlabel('X1'), plt.ylabel('X2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9V31PoNcFuO"
   },
   "source": [
    "## Decision trees out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBmN8Ildbe0d"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr-JTRwCcDR1"
   },
   "source": [
    "DecisionTreeClassifier has a number of parameters:\n",
    "* `max_depth` – a limit on tree depth (default – no limit)\n",
    "* `min_samples_split` – there should be at least this many samples to split further (default – 2)\n",
    "* `min_samples_leaf` – there should be at least this many samples on one side of a split to consider it valid (default – 1).\n",
    "* `criterion` – 'gini' or 'entropy' – split stuff over this parameter (default : gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KX2Dm7onbq2s"
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_toy, y_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM02fOjxcMic"
   },
   "source": [
    "### Plot decision surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBVDpZ6Y-jB5"
   },
   "source": [
    "Here's a function that makes a 2d decision boundary plot for a given classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGp3PLfW35hF"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def plot_decision_surface(\n",
    "                  clf, X, y,\n",
    "                  nx=200, ny=100,\n",
    "                  cmap='bwr',\n",
    "                  alpha=0.6,\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of clf on X and y, visualize training points\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the grid\n",
    "    x_top_left = X.min(axis=0) - 1\n",
    "    x_bottom_right = X.max(axis=0) + 1\n",
    "    grid_x0, grid_x1 = np.meshgrid(\n",
    "         np.linspace(x_top_left[0], x_bottom_right[0], ny),\n",
    "         np.linspace(x_top_left[1], x_bottom_right[1], nx)\n",
    "      )\n",
    "    \n",
    "    # Calculate predictions on the grid\n",
    "    y_pred_grid = clf.predict(\n",
    "                        np.stack(\n",
    "                              [\n",
    "                                grid_x0.ravel(),\n",
    "                                grid_x1.ravel()\n",
    "                              ],\n",
    "                              axis=1\n",
    "                            )\n",
    "                      ).reshape(grid_x1.shape)\n",
    "    \n",
    "    # Find optimal contour levels and make a filled\n",
    "    # contour plot of predictions\n",
    "    labels = np.sort(np.unique(y))\n",
    "    labels = np.concatenate([[labels[0] - 1],\n",
    "                             labels,\n",
    "                             [labels[-1] + 1]])\n",
    "    medians = (labels[1:] + labels[:-1]) / 2\n",
    "    plt.contourf(grid_x0, grid_x1, y_pred_grid, cmap=cmap, alpha=alpha,\n",
    "                 levels=medians)\n",
    "    \n",
    "    # Scatter data points on top of the plot,\n",
    "    # with different styles for correct and wrong\n",
    "    # predictions\n",
    "    y_pred = clf.predict(X)\n",
    "    plt.scatter(*X[y_pred==y].T, c=y[y_pred==y],\n",
    "                marker='o', cmap=cmap, s=10, label='correct')\n",
    "    plt.scatter(*X[y_pred!=y].T, c=y[y_pred!=y],\n",
    "                marker='x', cmap=cmap, s=50, label='errors')\n",
    "\n",
    "    # Dummy plot call to print the accuracy in the legend.\n",
    "    plt.plot([], [], ' ',\n",
    "             label='Accuracy = {:.3f}'.format(accuracy_score(y, y_pred)))\n",
    "    \n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0jDlaAXA1Ee"
   },
   "source": [
    "You may also check [sklearn.inspection.DecisionBoundaryDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9ODNVnC-jB9"
   },
   "source": [
    "Let's apply it to the tree we've fitted above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_2BZxCrb0tn"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plot_decision_surface(clf, X_toy, y_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RWucAGNcY-t"
   },
   "source": [
    "### Tree depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImiOildL-jCE"
   },
   "source": [
    "First we are going to split our data to train and test subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6clG_qq-jCE"
   },
   "outputs": [],
   "source": [
    "X_toy_train, X_toy_test, y_toy_train, y_toy_test = \\\n",
    "    train_test_split(X_toy, y_toy, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeL0PME0-jCI"
   },
   "source": [
    "Now it's your turn to investigate how the decision boundary depends on the tree depth. Maximum tree depth is defined by the `max_depth` parameter. Try out the following values: `[1, 2, 3, 5, 10]`. Make decision boundary plots for both train and test datasets (separately). **(2 points)**\n",
    "\n",
    "  > *Hint: you can make a nice plot with multiple columns and rows (see example below).*\n",
    "  \n",
    "```python\n",
    "plt.figure(figsize=(width, height))\n",
    "for i in range(num_rows * num_columns):\n",
    "  plt.subplot(num_rows, num_columns, i + 1)\n",
    "  # subplot numbering starts from 1   ^^^\n",
    "  \n",
    "  # ...\n",
    "  # (do the plotting)\n",
    "plt.show();\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppdUZd6U-jCI"
   },
   "outputs": [],
   "source": [
    "depth_values = [1, 2, 3, 5, 10]\n",
    "\n",
    "<YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lgCWhbEc77C"
   },
   "source": [
    "### Toy multiclass data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48yemIKk-jCQ"
   },
   "source": [
    "Now let's try out a multiclass classification case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wahVk0jegnb_"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/abdalazizrashid/MLDM-2023/main/07-trees/data.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7iUjzyo-jCU"
   },
   "source": [
    "Firstly, we'll load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2qKojahYhm1"
   },
   "outputs": [],
   "source": [
    "data = np.load('data.npz')\n",
    "X, y = data[\"X\"], data[\"y\"]\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cuuN5L6-jCb"
   },
   "source": [
    "And then split it to train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybPi8TNtYlgV"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.5, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5l3oJDs-jCi"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='rainbow', s = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6NA1BGH-jCo"
   },
   "source": [
    "Now that we've had a look at the data, let's fit a decision tree on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dvkVwtLZjDA"
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3s03yi0-jCs"
   },
   "source": [
    "and plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEHFL51JY02v"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 16))\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_decision_surface(clf, X_train, y_train, cmap='rainbow')\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_decision_surface(clf, X_test, y_test, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtKtjwBDdKZA"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmEoJSDxdHHs"
   },
   "source": [
    "#### We need a better tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwZ4BVxobkiu"
   },
   "source": [
    "Try adjusting the parameters of DecisionTreeClassifier to improve the test accuracy.\n",
    " * Accuracy >= 0.72 - not bad for a start\n",
    " * Accuracy >= 0.75 - better, but not enough\n",
    " * Accuracy >= 0.77 - pretty good\n",
    " * Accuracy >= 0.78 - great! (probably the best result for a single tree) **(1 point)**\n",
    " \n",
    "Feel free to modify the DecisionTreeClassifier above instead of re-writing everything.\n",
    "\n",
    "**Note:** some of the parameters you can tune are under the \"Decision trees out of the box\" header."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvm8lSZx-K5C"
   },
   "source": [
    "## Feature transformations\n",
    "Try adding feature transformations using a pipeline and a transformation, e.g. function transformer.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "clf = make_pipeline(\n",
    "    FunctionTransformer(lambda X: np.concatenate([X, X**2], axis=1)),\n",
    "    DecisionTreeClassifier()\n",
    ")\n",
    "```\n",
    "\n",
    "Which transformations should improve the score? **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCc2zNmcVOCz"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "clf = make_pipeline(\n",
    "FunctionTransformer(lambda X: X),\n",
    "DecisionTreeClassifier(random_state=42)\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(12, 16))\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_decision_surface(clf, X_train, y_train, cmap='rainbow')\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_decision_surface(clf, X_test, y_test, cmap='rainbow');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0sTlV_msz3H"
   },
   "source": [
    "```\n",
    "```\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvifS1edLmnd"
   },
   "source": [
    "We've talked a lot about the importance of feature scaling. Why aren't we doing it here?\n",
    "\n",
    "Try adding a standard scaler to the pipeline of your best model and check how it affects the result. Can you explain the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtF5Nsc_9uW0"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gX25hYFvL8DP"
   },
   "outputs": [],
   "source": [
    "<YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHT8J3bfLlWM"
   },
   "source": [
    "```\n",
    "```\n",
    "```\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciMVTsfrCzEz"
   },
   "source": [
    "# Bonus. Tree pruning (Minimal Cost-Complexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlV80NDkC2n9"
   },
   "source": [
    "Let tree $T$ have the total weighted sample impurity of the terminal nodes $R(T)$.\n",
    "\n",
    "Can prune the tree by minimizing:\n",
    "$$R_\\alpha(T) = R(T) + \\alpha\\left|T\\right|,$$\n",
    "where $\\alpha\\geq0$, and $\\left|T\\right|$ is the number of terminal nodes in the tree.\n",
    "\n",
    "Let $T_t$ be the subtree tree whose root node is $t\\in T$.\n",
    "\n",
    "$T_t$ will be pruned out (i.e. replaced with $t$ as the terminal node) if:\n",
    "$$R(t)+\\alpha < R(T_t)+\\alpha\\left|T_t\\right|$$\n",
    "or in other words if:\n",
    "$$\\alpha > \\alpha_{eff}(t)=\\frac{R(t) - R(T_t)}{\\left|T_t\\right|-1}$$\n",
    "\n",
    "One can use the `cost_complexity_pruning_path` method of `DecisionTreeClassifier` to get the list of $\\alpha_{eff}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yg80TYO9FXVn"
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "plt.plot(ccp_alphas, impurities, marker='o', drawstyle=\"steps-post\")\n",
    "plt.xlabel(\"effective alpha\")\n",
    "plt.ylabel(\"total impurity of leaves\")\n",
    "plt.title(\"Total Impurity vs effective alpha for training set\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UONd5irJBnV"
   },
   "source": [
    "Your turn! `DecisionTreeClassifier` has a `ccp_alpha` parameter to prune the tree.\n",
    "\n",
    " For each of the `ccp_alphas` defined above fit a tree, and then make 3 plots:\n",
    " - tree depth vs alpha\n",
    " - number of nodes in the tree vs alpha\n",
    " - train and test accuracies vs alpha\n",
    "\n",
    "You can get the tree depth with `clf.tree_.max_depth`, and number of nodes with `clf.tree_.node_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QD2XuoXpGoJd"
   },
   "outputs": [],
   "source": [
    "tree_depth = []\n",
    "number_of_nodes = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for alpha in ccp_alphas:\n",
    "# YOUR CODE <>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhUHlwfkqI1i"
   },
   "source": [
    "## Visualizing a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tz9q62nkhBnU"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "plot_tree(clf, max_depth=2, fontsize=10, filled=True);"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
